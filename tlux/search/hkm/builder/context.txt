Directory structure
===============================

builder/
|___ agents.md
|___ launcher.py
|___ metadata_merger.py
|___ partitioner.py
|___ recursive_index_builder.py
|___ tokenize_and_embed.py


File contents
==============

--- Start of file: agents.md ---
```
# Coding Guidelines

## Core Principles
- **Lean & self-contained** – standard library + NumPy only is preferred; add dependencies only with a material and quantified benefit.
- **Pedagogical clarity** – names and comments teach; code is decipherable without a debugger.
- **Deterministic efficiency** – predictable memory/CPU, seeded RNGs, streaming where possible.
- **Maintainability first** – optimize after profiling, not before.
- **ASCII-only source** – no Unicode or special characters inside code or comments.

## Layout & Naming
- **Banner (triple-quoted)** – 1-sentence synopsis,  <= 30-line overview, example usage.
- **Imports** – stdlib -> third-party -> intra-package, one per line, no wildcards.
- **Constants / private helpers** – `_snake_case`, `UPPER_SNAKE` for constants.
- **Typing** – functions use full type hints on all inputs and output.
- **Naming** – `snake_case.py`, `PascalCase`, `snake_case`, `_private`, `UPPER_SNAKE`, CLI flags like `--max-iter`.
- **CLI & demo** – under `if __name__ == "__main__":`, <= 25 LOC, zero side-effects on import.

## Documentation & Comments
- **Module banner** as above.
- **API docs** – NumPy-style comment blocks with "#" *preceding* classes / functions  
  (`# Description:\n#  ...\n# \n# Parameters:\n#   arg (type): description\n# ...`), no inline `"""docstrings"""` inside definitions.
- Inline comments provide "overview descriptions" for intents of blocks of code and fill explanations where reading the code alone is not sufficiently obvious to indicate why something is being done.

## API, Types & Error Contracts
- PEP 484 everywhere public; prefer concrete dtypes (`np.float32`, `np.int32`) and note shapes `(n, d)`.
- Validate inputs early, raise `ValueError`, `TypeError`, or `RuntimeError` with explicit messages.
- Use the message to state the violated contract (“k must be in [1, n]”).

## Performance & I/O
- Use seeded random generators with default values.
- Minimize memory footprint where possible, vectorize with NumPy; memory-map large arrays.
- O(1) or single-pass algorithms are preferred; profile before micro-optimizing.
- Avoid `print` unless in demonstration code; internal error logs use `logging`.

## Testing & Examples
- Each algorithm has a doctest-style example in the preceding comments.
- Unit tests reside in local `./tests/` subdirectories and not inside source files.
- Every file should be runnable as a quick sanity check via its `__main__` if it does not already have a command line to support.
```
--- End of file: agents.md ---


--- Start of file: launcher.py ---
```
# Default driver orchestrator for HKM index construction.
# 
# This module spawns separate processes for:
#   1. Tokenizing and embedding raw documents in parallel workers.
#   2. Building the hierarchical K-Means (HKM) tree once all shards are ready.
# 
# Workers are launched via `jobs.spawn_job`, which invokes a Python function in
# a new process with the provided arguments and optional dependencies.
# 
# Example usage:
#   python launcher.py /path/to/index /path/to/docs --workers 4


import os
import argparse

from ..fs import FileSystem
from .jobs import spawn_job


# Function to build the HKM tree by orchestrating worker processes
#
# Prepares the directory structure, spawns worker processes to tokenize and
# embed documents, and then spawns a job to build the HKM tree once all
# workers complete.
#
# Parameters:
#   fs (FileSystem): The file system object for directory operations.
#   docs_dir (str): Path to the directory containing raw text documents.
#   index_root (str): Root directory where the HKM index will be created.
#   num_workers (int): Number of parallel worker processes.
#
# Returns:
#   None
#
# Raises:
#   ValueError: If docs_dir does not exist or num_workers is not a positive integer.
# 
def build_hkm_tree(
    fs: FileSystem,
    docs_dir: str,
    index_root: str,
    num_workers: int,
) -> None:
    # Validate input parameters
    if not fs.exists(docs_dir):
        raise ValueError(f"docs_dir '{docs_dir}' does not exist")
    if not isinstance(num_workers, int) or num_workers <= 0:
        raise ValueError("num_workers must be a positive integer")

    # Create subdirectories for document shards and HKM tree
    docs_root_out = fs.join(index_root, "docs")
    fs.mkdir(docs_root_out, exist_ok=True)
    hkm_root = fs.join(index_root, "hkm")
    fs.mkdir(hkm_root, exist_ok=True)

    # Launch worker processes to tokenize and embed documents
    worker_jobs = []
    for worker_id in range(num_workers):
        work_dir = fs.join(docs_root_out, f"worker_{worker_id:04d}")
        job = spawn_job(
            "hkm.builder.tokenize_and_embed.default_worker",
            document_directory=docs_dir,
            output_directory=work_dir,
            worker_index=worker_id,
            total_workers=num_workers,
        )
        worker_jobs.append(job)

    # Launch the HKM tree builder once all worker jobs are complete
    spawn_job(
        "hkm.builder.recursive_index_builder.build_cluster_index",
        index_root,
        dependencies=worker_jobs,
    )


# Entry point for the HKM driver
#
# Description:
#   Parses command-line arguments, initializes the file system, and dispatches
#   worker and tree-building jobs.
#
def main() -> None:
    parser = argparse.ArgumentParser(
        description="Orchestrate tokenization, embedding, and HKM build"
    )
    parser.add_argument(
        "index_root",
        help="Root directory where the HKM index will be created",
    )
    parser.add_argument(
        "docs_dir",
        help="Path to the directory containing raw text documents",
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=os.cpu_count() or 4,
        help="Number of parallel tokenization/embedding workers",
    )
    args = parser.parse_args()

    fs = FileSystem()
    build_hkm_tree(fs, args.docs_dir, args.index_root, args.workers)


if __name__ == "__main__":  # pragma: no cover
    main()


```
--- End of file: launcher.py ---

--- Start of file: metadata_merger.py ---
```
# Worker that reads all metadata from shards in provided docs directory
# and writes a unified metadata file to the current directory.
# 
# Overview:
# - Scans all `worker_*` directories under `docs_root`.
# - Reads all `.meta.npy` files within each worker directory.
# - Concatenates their contents into a single array.
# - Writes the merged array to `out_path` using np.memmap.
# 
# Example usage:
#   $ python merge_metadata.py ./docs ./merged.meta.npy
# 

import argparse
import logging
from pathlib import Path

import numpy as np

from ..fs import FileSystem
from ..schema import DOC_META_DTYPE


# Description:
#   Merge all .meta.npy arrays under subdirectories of a given root
#   and save a unified metadata file using memory mapping.
#
# Parameters:
#   docs_root (str): Path to the root directory containing worker subdirectories.
#   out_path (str): Path to save the merged metadata file.
#
# Raises:
#   ValueError: If docs_root does not exist or contains no metadata files.
def merge_metadata(docs_root: str, out_path: str) -> None:
    fs = FileSystem()
    metadata_dir = docs_root + "/metadata"
    # Validate inputs early
    if not fs.exists(docs_root):
        raise ValueError(f"docs_root does not exist: {docs_root}")
    if not fs.exists(metadata_dir):
        raise ValueError(f"docs_root metadata directory does not exist: {metadata_dir}")

    all_meta = []

    # Collect all .meta.npy files within this worker directory
    for meta_file in fs.listdir(metadata_dir ):
        arr = np.load(str(meta_file), mmap_mode="r")
        all_meta.append(arr)

    # Concatenate all metadata arrays (empty array fallback)
    if all_meta:
        merged = np.concatenate(all_meta, axis=0)
    else:
        merged = np.empty((0,), dtype=DOC_META_DTYPE)

    # Write the result using memory-mapped I/O
    mm = np.memmap(out_path, dtype=DOC_META_DTYPE, shape=merged.shape, mode="w+")
    mm[:] = merged
    mm.flush()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Merge all per-shard metadata into one file."
    )
    parser.add_argument("docs_root", type=str, help="Directory containing worker_* subdirectories")
    parser.add_argument("out_path", type=str, help="Path to write merged metadata file")
    args = parser.parse_args()

    merge_metadata(args.docs_root, args.out_path)

```
--- End of file: metadata_merger.py ---

--- Start of file: partitioner.py ---
```
"""
A worker whose job is to steadily consume files from a doc directory,
 accumulate summary staticis across reads, and steadily write docs to
 appropriate cluster_XXXX/doc/ directories based on proximity.

Reserves files by moving them to our own working directory ./docs/worker_XXXX/
and writes aggregate summary statistic information (categorical counts, numeric
percentiles, and bloom filters) to ./docs/worker_XXXX/ including the files:
  category_map.json  --  Category label counts.
  numeric_map.json   --  Numeric percentiles [0, 100].
  hashbitmask.bin    --  Bloom filter over n-grams of tokens in docs scanned.

"""

```
--- End of file: partitioner.py ---

--- Start of file: recursive_index_builder.py ---
```
# Builds a hierarchical k-means (HKM) index for a specific cluster.
# 
# This script recursively constructs an HKM index tree by processing document embeddings.
# It performs the following steps:
#  1. Initiates a background job to merge metadata from all documents in the cluster.
#  2. Collects all document embeddings from files using memory mapping.
#  3. Selects small random and diverse sets of embedding indices for quick previews.
#  4. Executes k-means clustering to determine cluster centers (centroids).
#  5. Spawns jobs to partition documents into clusters based on these centroids.
#  6. Triggers new index-building jobs for each sub-cluster, continuing the recursion.
# 
# Notes:
# - Assumes the number of embeddings per cluster fits in memory.
# - For large clusters, sampling could be considered to reduce memory usage.
# - Recursion halts when an external condition deems clusters sufficiently small.
# 
# Example usage:
#     $ python launcher.py /path/to/index_root


import argparse
from pathlib import Path
from typing import List, Any

import numpy as np

from ..fs import FileSystem
from ..jobs import spawn_job
from ..tools.kmeans import kmeans
from ..tools.preview import select_random, select_diverse
from .metadata_merger import _run_merge_metadata


# Constants
MAX_PREVIEW_SIZE: int = 512
MAX_CLUSTER_COUNT: int = 1024
RANDOM_SEED: int = 42


# Builds the HKM index for the specified cluster.
# 
# Parameters
# ----------
# index_root_directory : str
#     The root directory containing the index structure.
# 
# Raises
# ------
# TypeError
#     If index_root_directory is not a string.
# ValueError
#     If the specified directory does not exist.
# 
# Notes
# -----
# This function uses memory-mapped NumPy arrays to efficiently handle large embedding files.
# 
def build_cluster_index(index_root_directory: str) -> None:
    # Input validation
    if not isinstance(index_root_directory, str):
        raise TypeError("index_root_directory must be a string")
    root_path: Path = Path(index_root_directory)
    if not root_path.exists():
        raise ValueError(f"Directory does not exist: {index_root_directory}")

    # Initialize filesystem and define directory paths
    filesystem: FileSystem = FileSystem()
    docs_dir: str = filesystem.join(index_root_directory, "docs")
    hkm_dir: str = filesystem.join(index_root_directory, "hkm")
    filesystem.mkdir(hkm_dir, exist_ok=True)

    # Collect embedding file paths
    embedding_paths: List[Path] = list(Path(docs_dir).rglob("embed_*.npy"))
    if not embedding_paths:
        raise ValueError(f"No embedding files found in {docs_dir}")

    # Load embeddings with memory mapping for efficiency
    embedding_arrays: List[np.ndarray] = [
        np.load(str(path), mmap_mode="r") for path in embedding_paths
    ]
    combined_embeddings: np.ndarray = np.vstack(embedding_arrays) if embedding_arrays else np.empty((0, embedding_arrays[0].shape[1]), dtype=np.float32)

    # Calculate preview sample size based on data size
    preview_size: int = min(combined_embeddings.shape[0], MAX_PREVIEW_SIZE)

    # Generate preview indices with deterministic randomness
    random_indices: np.ndarray = select_random(
        range(combined_embeddings.shape[0]), preview_size, seed=RANDOM_SEED
    )
    diverse_indices: np.ndarray = select_diverse(
        combined_embeddings, preview_size, seed=RANDOM_SEED
    )

    # Save preview indices to disk
    np.save(filesystem.join(hkm_dir, "preview_random.npy"), random_indices)
    np.save(filesystem.join(hkm_dir, "preview_diverse.npy"), diverse_indices)

    # Determine maximum cluster count
    cluster_limit: int = min(combined_embeddings.shape[0], MAX_CLUSTER_COUNT)

    # Perform k-means clustering with seeded RNG
    cluster_centers: np.ndarray
    cluster_centers, _ = kmeans(combined_embeddings, cluster_limit, seed=RANDOM_SEED)
    np.save(filesystem.join(hkm_dir, "centroids.npy"), cluster_centers)

    # Spawn partitioning jobs for each cluster
    assignment_jobs: List[Any] = []
    for cluster_id in range(cluster_centers.shape[0]):
        job = spawn_job(
            "hkm.builder.worker_partitioner",
            docs_dir,
            hkm_dir,
            cluster_id=cluster_id,
            num_clusters=cluster_centers.shape[0],
        )
        assignment_jobs.append(job)
    # Launch recursive index-building jobs
    for cluster_id in range(cluster_centers.shape[0]):
        sub_hkm_dir: str = filesystem.join(hkm_dir, f"cluster_{cluster_id:04d}")
        spawn_job(
            "hkm.builder.recursive_index_builder.build_cluster_index",
            sub_hkm_dir,
            dependencies=assignment_jobs,
        )
    # Initiate metadata merging job for *this* cluster.
    spawn_job(
        "hkm.builder.worker_metadata_merger",
        docs_dir,
        hkm_dir,
    )



if __name__ == "__main__":
    """Command-line interface for building an HKM index."""
    parser = argparse.ArgumentParser(description="Build an HKM index tree for a cluster.")
    parser.add_argument(
        "index_root_directory",
        type=str,
        help="Root directory for the index structure"
    )
    args = parser.parse_args()
    build_cluster_index(args.index_root_directory)

```
--- End of file: recursive_index_builder.py ---

--- Start of file: tokenize_and_embed.py ---
```
# worker_tokenize_embed.py
#
# This module processes documents by tokenizing and embedding them in a single pass,
# organizing the results into size-limited chunk files (.hkmchunk).
#
# Key features:
#   - Processes documents once: reads document_batches only once, building data in memory
#     and saving it to disk when a chunk reaches its size limit.
#   - Memory-efficient: buffers all data in RAM using ChunkBuffer, avoiding temporary
#     disk writes until a chunk is complete.
#   - Chunk contents:
#     - tokens_{i:08d}.bin: token sequences as packed 32-bit integers
#     - embeddings.mmap: float32 embeddings for all windows
#     - embed_index.mmap: array describing each embedding (doc_id, start, end, window_size)
#     - categories.mmap: int64 categorical metadata per document
#     - numbers.mmap: float64 numerical metadata per document
#   - Metadata schema: defined by the user as a list of (field_name, type) pairs,
#     where type is str or float, setting the number of categorical and numerical fields.
#   - Chunk naming: files are named chunk_<min_doc_id>_<max_doc_id>.hkmchunk based on
#     document ID range.
#   - Global outputs: category_map.json maps categorical values to integers, and
#     numeric_map.json provides percentiles for numerical values.
#   - N-gram tracking: includes a placeholder for counting unique n-grams (not implemented).
#
# Usage:
#     process_documents(
#         output_directory: str,
#         document_batches: Iterable[([texts], [infos])],
#         metadata_schema: List[Tuple[field_name, type]],
#         chunk_size_limit: int = 8 * 2**20,
#     )

import os
import argparse
import io
import json
import hashlib
import struct
import tempfile
import zipfile
from io import BytesIO
from pathlib import Path
from typing import Iterable, Tuple, Union, List, Dict

import numpy as np
from ..fs import FileSystem
from ..embedder import tokenize, embed_windows
from .unique_count_estimator import UniqueCounter

# Type definitions for better readability
DocumentBatch = Iterable[Tuple[List[str], List[List[Union[str, float]]]]]
MetadataSchema = List[Tuple[str, type]]

# Format for embedding index entries: document ID, token start, token end, window size
EMBEDDING_INDEX_TYPE = np.dtype([
    ('document_id', np.uint32),
    ('token_start', np.uint32),
    ('token_end', np.uint32),
    ('window_size', np.uint32),
])

# Class to buffer document data in memory before saving it to disk.
# Manages tokens, embeddings, and metadata, writing them to a chunk file when the
# size limit is reached.
class ChunkBuffer:
    BUFFER_SIZE = 8 * 1024  # 8 KB buffer for writing data

    def __init__(self, file_system: FileSystem, output_directory: str, chunk_size_limit: int, metadata_schema: MetadataSchema):
        self.file_system = file_system
        self.output_directory = output_directory
        self.chunk_size_limit = chunk_size_limit
        self.metadata_schema = metadata_schema
        self._setup_new_buffers()
        self.document_count = 0

    def _setup_new_buffers(self):
        # Initialize tracking for document IDs and data size
        self.min_document_id = float('inf')
        self.max_document_id = -1
        self.total_bytes = 0
        # Track where token data starts and its size
        self.token_offsets: List[int] = []
        self.token_sizes: List[int] = []
        # Set up temporary files and buffers for data
        self.token_file = tempfile.TemporaryFile()
        self.token_writer = io.BufferedWriter(self.token_file, buffer_size=self.BUFFER_SIZE)
        self.embedding_file = tempfile.TemporaryFile()
        self.embedding_writer = io.BufferedWriter(self.embedding_file, buffer_size=self.BUFFER_SIZE)
        self.index_file = tempfile.TemporaryFile()
        self.index_writer = io.BufferedWriter(self.index_file, buffer_size=self.BUFFER_SIZE)
        self.category_file = tempfile.TemporaryFile()
        self.category_writer = io.BufferedWriter(self.category_file, buffer_size=self.BUFFER_SIZE)
        self.number_file = tempfile.TemporaryFile()
        self.number_writer = io.BufferedWriter(self.number_file, buffer_size=self.BUFFER_SIZE)

    def add_document(
        self,
        document_id: int,
        tokens: List[int],
        embeddings: np.ndarray,
        embedding_windows: List[Tuple[int, int, int]],
        metadata: List[Union[int, float]],
    ):
        # Save token sequence
        token_data = struct.pack(f"<{len(tokens)}I", *tokens)
        self.token_writer.flush()  # Make sure position is current
        offset = self.token_file.tell()
        self.token_offsets.append(offset)
        self.token_sizes.append(len(token_data))
        self.token_writer.write(token_data)
        self.total_bytes += len(token_data)

        # Track document ID range
        self.min_document_id = min(self.min_document_id, document_id)
        self.max_document_id = max(self.max_document_id, document_id)

        # Save embeddings and their index entries
        for i, (start, end, window_size) in enumerate(embedding_windows):
            embedding_row = embeddings[i]
            self.embedding_writer.write(embedding_row.tobytes())
            self.total_bytes += embedding_row.nbytes

            index_entry = struct.pack("<4I", document_id, start, end, window_size)
            self.index_writer.write(index_entry)
            self.total_bytes += EMBEDDING_INDEX_TYPE.itemsize

        # Split metadata into categorical and numerical parts
        category_values: List[int] = []
        number_values: List[float] = []
        for (field_name, field_type), value in zip(self.metadata_schema, metadata):
            if field_type is float:
                number_values.append(float(value))
            else:
                category_values.append(int(value))  # Already an integer from mapping

        # Save categorical data
        if category_values:
            packed_categories = struct.pack(f"<{len(category_values)}q", *category_values)
            self.category_writer.write(packed_categories)
            self.total_bytes += len(packed_categories)
        # Save numerical data
        if number_values:
            packed_numbers = struct.pack(f"<{len(number_values)}d", *number_values)
            self.number_writer.write(packed_numbers)
            self.total_bytes += len(packed_numbers)

        # Write to disk if size limit is reached
        if self.total_bytes >= self.chunk_size_limit:
            self.save_chunk()

    def save_chunk(self):
        if self.min_document_id == float('inf'):
            return  # Nothing to save

        # Ensure all data is written to temporary files
        for writer in (self.token_writer, self.embedding_writer, self.index_writer,
                       self.category_writer, self.number_writer):
            writer.flush()

        # Create a ZIP file in memory
        zip_buffer = BytesIO()
        with zipfile.ZipFile(zip_buffer, 'w', compression=zipfile.ZIP_STORED) as zip_file:
            # Save each document's tokens separately
            for i, (offset, size) in enumerate(zip(self.token_offsets, self.token_sizes)):
                self.token_file.seek(offset)
                data = self.token_file.read(size)
                zip_file.writestr(f"tokens_{i:08d}.bin", data)

            # Save all embeddings, index, categories, and numbers for the chunk
            for filename, temp_file in [
                ("embeddings.mmap", self.embedding_file),
                ("embed_index.mmap", self.index_file),
                ("categories.mmap", self.category_file),
                ("numbers.mmap", self.number_file),
            ]:
                temp_file.seek(0)
                data = temp_file.read()
                if data:
                    zip_file.writestr(filename, data)

        # Write the ZIP to disk
        chunk_name = f"chunk_{int(self.min_document_id):08d}_{int(self.max_document_id):08d}.hkmchunk"
        with self.file_system.open(os.path.join(self.output_directory, chunk_name), 'wb') as output_file:
            output_file.write(zip_buffer.getvalue())

        # Clean up and prepare for next chunk
        for writer in (self.token_writer, self.embedding_writer, self.index_writer,
                       self.category_writer, self.number_writer):
            writer.close()
        self._setup_new_buffers()

# Function to process documents by tokenizing, embedding, and organizing them into chunks.
# Handles everything in one pass, saving data to disk when chunks are full.
# Parameters:
# - output_directory: Where to save chunk files.
# - document_batches: Source of document texts and metadata.
# - metadata_schema: Defines metadata fields and their types (str or float).
# - chunk_size_limit: Maximum size in bytes for each chunk.
def process_documents(
    output_directory: str,
    document_batches: DocumentBatch,
    metadata_schema: MetadataSchema,
    chunk_size_limit: int = 8 * 2**20,
    n_gram: int = 3,
) -> None:
    file_system = FileSystem()
    file_system.mkdir(output_directory, exist_ok=True)

    # Count how many fields are categorical or numerical
    categorical_field_count = sum(1 for (name, typ) in metadata_schema if typ is str)
    numerical_field_count = sum(1 for (name, typ) in metadata_schema if typ is float)

    # Track categories and their frequencies globally
    category_ids: Dict[str, int] = {}
    category_counts: Dict[str, int] = {}
    # Collect numerical values for percentiles
    number_values: Dict[str, List[float]] = {name: [] for name, typ in metadata_schema if typ is float}

    chunk_buffer = ChunkBuffer(file_system, output_directory, chunk_size_limit, metadata_schema)
    document_id = 0
    ngram_counter = UniqueCounter()  # Placeholder for unique n-gram counting

    for texts, metadata_list in document_batches:
        for text, metadata in zip(texts, metadata_list):
            # Convert text to tokens
            tokens = tokenize([text])[0]
            # Add the n-grams to the counter.
            for n in range(1, n_gram + 1):
                for i in range(len(tokens) - n + 1):
                    ngram_bytes = b''.join(token.to_bytes() for token in tokens[i:i+n])
                    ngram_counter.add(ngram_bytes)
            # Create embeddings for token windows
            embeddings, embedding_windows = embed_windows([tokens])
            # Process metadata into integers (categories) and floats (numbers)
            processed_metadata = []
            for (field_name, field_type), value in zip(metadata_schema, metadata):
                if field_type is float:
                    processed_metadata.append(float(value))
                    number_values[field_name].append(value)
                else:
                    key = str((field_name, value))
                    if key not in category_ids:
                        # Create a unique integer ID for this category
                        hash_value = int.from_bytes(hashlib.sha256(key.encode()).digest()[:8], 'little')
                        category_ids[key] = hash_value
                        category_counts[key] = 1
                    else:
                        category_counts[key] += 1
                    processed_metadata.append(category_ids[key])
            # Add document data to buffer
            chunk_buffer.add_document(document_id, tokens, embeddings, embedding_windows, processed_metadata)
            document_id += 1

    # Save any remaining data
    chunk_buffer.save_chunk()
    # Save unique n-gram count.
    with file_system.open(file_system.join(output_directory, 'n_gram_counter.bytes'), 'w') as output_file:
        output_file.write(ngram_counter.dumps())
    # Save category mappings
    category_data = [(key, (category_ids[key], category_counts[key])) for key in category_ids]
    with file_system.open(file_system.join(output_directory, 'category_map.json'), 'w') as output_file:
        json.dump(category_data, output_file, indent=2)
    # Save numerical percentiles
    number_data = []
    for field_name, values in number_values.items():
        if values:
            percentiles = np.percentile(values, np.linspace(0, 100, 101)).tolist()
            number_data.append((field_name, percentiles))
    with file_system.open(file_system.join(output_directory, 'numeric_map.json'), 'w') as output_file:
        json.dump(number_data, output_file, indent=2)


# Default worker called by invocation of `main()`.
def default_worker(
    document_directory: str,
    output_directory: str,
    metadata_schema: str = "[]",
    worker_index: int = 0,
    total_workers: int = 1,
):
    # Convert metadata schema from JSON
    schema = json.loads(metadata_schema)
    metadata_schema = [(name, str if typ == 'str' else float) for name, typ in schema]

    # Find and split document files among workers
    all_files = sorted(Path(document_directory).glob('*'))
    my_files = [
        file for i, file in enumerate(all_files)
        if i % total_workers == worker_index
    ]

    # Create an iterator for document batches
    def get_document_batches():
        for file in my_files:
            text = file.read_text(encoding='utf-8')
            # Simplified: no metadata; real use would include metadata here
            yield [text], [[]]

    process_documents(
        output_directory,
        get_document_batches(),
        metadata_schema,
        chunk_size_limit=chunk_size_limit,
    )


# Main function to handle command-line inputs and start processing.
def main():
    parser = argparse.ArgumentParser(
        description="Worker script to tokenize and embed documents with metadata"
    )
    parser.add_argument('document_directory', help='Folder with document files')
    parser.add_argument('output_directory', help='Folder to save chunk files')
    parser.add_argument('--metadata_schema', type=str, default='[]',
                        help='JSON list of [field_name, "str" or "float"] pairs')
    parser.add_argument('--chunk_size_limit', type=int, default=8 * 2**20,
                        help='Maximum chunk size in bytes')
    parser.add_argument('--worker_index', type=int, default=0,
                        help='Index of this worker (0-based)')
    parser.add_argument('--total_workers', type=int, default=1,
                        help='Number of workers running in parallel')
    args = parser.parse_args()
    default_worker(
        document_adirectory=args.document_directory,
        output_directory=args.output_directory,
        metadata_schema=args.metadata_schema,
        worker_index=args.worker_index,
        total_workers=args.total_workers,
    )



if __name__ == '__main__':
    main()

```
--- End of file: tokenize_and_embed.py ---
